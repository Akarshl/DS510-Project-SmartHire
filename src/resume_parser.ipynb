{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_224Lwgl8Yx2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#run only once\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCfqUzE-8hL8",
        "outputId": "9a37e9a6-8d8b-4f36-ed12-2c80eb25eafb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_email(text):\n",
        "  matches = re.findall(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+', text)\n",
        "  return matches[0] if matches else None"
      ],
      "metadata": {
        "id": "iyl6s6Gp8omU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_phone(text):\n",
        "  matches = re.findall(r'(\\(?\\d{3}\\)?[-.\\s]?){2}\\d{4}', text)\n",
        "  return \"\".join(matches[0]) if matches else None"
      ],
      "metadata": {
        "id": "iHjbaW-I8r01"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  tokens = word_tokenize(text)\n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  words = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
        "  return words"
      ],
      "metadata": {
        "id": "w0FlVvjw8uj0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_resume_row(row):\n",
        "  text = row['Resume'] or \"\"\n",
        "  result = {\n",
        "      \"category\": row['Category'],\n",
        "      \"email\": extract_email(text),\n",
        "      \"phone\": extract_phone(text),\n",
        "      \"cleaned_tokens\": clean_text(text),\n",
        "      \"raw_text_preview\": text[:500]\n",
        "      }\n",
        "  return result"
      ],
      "metadata": {
        "id": "qjm98UOn8xe0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_csv(csv_path, output_path):\n",
        "  df = pd.read_csv(csv_path)\n",
        "  results = []"
      ],
      "metadata": {
        "id": "Yu-VdvjPCabi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_csv(csv_path, output_path):\n",
        "  df = pd.read_csv(csv_path)\n",
        "  results = []\n",
        "  for _, row in df.iterrows():\n",
        "    try:\n",
        "        parsed = process_resume_row(row)\n",
        "        results.append(parsed)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process row: {e}\")\n",
        "\n",
        "  with open(output_path, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "  print(f\"Processed {len(results)} resumes into {output_path}\")"
      ],
      "metadata": {
        "id": "j6mAFwoI80sB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  csv_input = \"/content/UpdatedResumeDataSet.csv\"\n",
        "  json_output = \"parsed_resume_data.json\"\n",
        "  process_csv(csv_input, json_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGoD9YCR88CU",
        "outputId": "863c38e3-4863-45d4-8060-78419c2ddfa9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 962 resumes into parsed_resume_data.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0363c69"
      },
      "source": [
        "def process_csv(csv_path, output_path):\n",
        "  df = pd.read_csv(csv_path)\n",
        "  results = []\n",
        "  for _, row in df.iterrows():\n",
        "    try:\n",
        "        parsed = process_resume_row(row)\n",
        "        results.append(parsed)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process row: {e}\")\n",
        "\n",
        "  with open(output_path, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "  print(f\"Processed {len(results)} resumes into {output_path}\")"
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}